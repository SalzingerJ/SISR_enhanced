{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 AIT Austrian Institute of Technology GmbH\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# Author: Miguel Castells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a66a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms.functional as Fvis\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import match_histograms\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc499a0",
   "metadata": {},
   "source": [
    "# Manual pick the low resolution images with worst PSNR after generation \n",
    "## Model used for generation :\n",
    "* Pretraining : SEN2NAIPv2-UNET\n",
    "* Finetuning : Our dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as file:\n",
    "        res = json.load(file)\n",
    "    return res\n",
    "\n",
    "def extract_metrics(results_dict, metric_name):\n",
    "    \"\"\"\n",
    "    results_dict: dict like your example, keys = image/folder names, values = dict of metrics\n",
    "    metric_name: str, e.g. 'psnr_rgb', 'clip_rgb', etc.\n",
    "\n",
    "    Returns:\n",
    "        sorted_metrics: list of tuples (image_name, metric_value), sorted descending by metric_value\n",
    "    Also plots histogram of metric values with mean line and label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract (name, metric_value) pairs, ignoring entries without the metric\n",
    "    data = [(name, metrics[metric_name]) for name, metrics in results_dict.items() if metric_name in metrics]\n",
    "\n",
    "    # Sort by metric descending\n",
    "    sorted_metrics = sorted(data, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_metrics\n",
    "\n",
    "def get_worst_chips_by_threshold(psnr_list, threshold):\n",
    "    \"\"\"\n",
    "    psnr_list: list of tuples (chip_name, psnr)\n",
    "    threshold: float, PSNR value below which chips are considered \"worst\"\n",
    "\n",
    "    Returns:\n",
    "        list of chip names with PSNR < threshold\n",
    "    \"\"\"\n",
    "    if not psnr_list:\n",
    "        return []\n",
    "\n",
    "    worst = [chip for chip, psnr in psnr_list if psnr < threshold]\n",
    "    return worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20aa485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hr_path(hr_root, chip):\n",
    "    \"\"\"\n",
    "    Search recursively in HR folder to find 'rgb.png' under a directory ending with chip.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(hr_root):\n",
    "        if chip in root.split(os.sep) and 'rgb.png' in files:\n",
    "            return os.path.join(root, 'rgb.png')\n",
    "    return None\n",
    "\n",
    "def get_list_unique_id_chip(chips, hr_root):\n",
    "    \"\"\"\n",
    "    Return list of unique_id/chip for a given list of chip names.\n",
    "    \"\"\"\n",
    "    identifiers = []\n",
    "    for chip in chips:\n",
    "        hr_path = find_hr_path(hr_root, chip)\n",
    "        \n",
    "        if hr_path:\n",
    "            identifiers.append(os.path.join(hr_path.split('/')[-3], chip))\n",
    "        else:\n",
    "            print(f\" Missing image for chip: {chip}\")\n",
    "    return identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43affb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(unique_id_chip, rgb_dir, nir_dir):\n",
    "    '''\n",
    "    Loads the high-resolution (NAIP) and low-resolution (Sentinel-2) images for both RGB and NIR channels,\n",
    "    given a unique ID and chip identifier.\n",
    "\n",
    "    Args:\n",
    "        unique_id_chip (str): A string formatted as \"<unique_id>/<chip_coord>\".\n",
    "        valid_index (list): List of valid LR indices to extract from the LR tensor.\n",
    "        rgb_dir (str): Root directory containing NAIP and Sentinel-2 RGB images.\n",
    "        nir_dir (str): Root directory containing NAIP NIR images.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the high-resolution image tensor (HR)\n",
    "        and the selected low-resolution image tensors (LRS), both with RGB + NIR channels.\n",
    "    '''\n",
    "    unique_id, chip_coord = unique_id_chip.split('/')[0], unique_id_chip.split('/')[1]\n",
    "    hr_path = os.path.join(rgb_dir, 'naip', unique_id, chip_coord, 'rgb.png')\n",
    "    lr_path = os.path.join(rgb_dir, 'sentinel2', chip_coord, 'tci.png')\n",
    "\n",
    "    nir_lr_path = str(lr_path.replace('tci.png', 'b08.png'))\n",
    "    lr_nir = Fvis.pil_to_tensor(Image.open(nir_lr_path))\n",
    "    lrs_nir = torch.reshape(lr_nir, (-1, 32,32)).unsqueeze(0).permute(1,0,2,3)\n",
    "\n",
    "    nir_hr_path = os.path.join(nir_dir, 'naip', unique_id, chip_coord, 'nir.png')\n",
    "    hr_nir = Fvis.pil_to_tensor(Image.open(nir_hr_path)) \n",
    "\n",
    "    hr_rgb = Fvis.pil_to_tensor(Image.open(hr_path))\n",
    "    lr_tensor = Fvis.pil_to_tensor(Image.open(lr_path))\n",
    "    lrs_rgb = torch.reshape(lr_tensor, (3,-1, 32,32)).permute(1,0,2,3)\n",
    "\n",
    "    lrs = torch.cat([lrs_rgb, lrs_nir], dim=1)\n",
    "    hr = torch.cat([hr_rgb, hr_nir], dim=0)\n",
    "\n",
    "    return hr, lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_1_cloud_removal(hr, lrs, device, num_imgs=16):\n",
    "    '''\n",
    "    Performs a naive cloud-removal step by selecting the `num_imgs_m1` best-matching low-resolution (LR)\n",
    "    images using Peak Signal-to-Noise Ratio (PSNR) between bicubic-downsampled HR and LR images.\n",
    "\n",
    "    Args:\n",
    "        hr (torch.Tensor): High-resolution image tensor (C, H, W).\n",
    "        lrs (torch.Tensor): Low-resolution image tensor batch (N, C, H, W).\n",
    "        num_imgs_m1 (int): Number of LR patches to keep.\n",
    "        device (str): Device string (\"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Indices of the top `num_imgs_m1` LR images.\n",
    "    '''\n",
    "    lrs = lrs.to(device)\n",
    "    num_lr = lrs.shape[0]\n",
    "    hr_res = F.interpolate(hr.unsqueeze(0).float(), (32,32), mode='bicubic').to(device)\n",
    "    hrs_res = hr_res.expand((num_lr, 4, 32, 32))\n",
    "\n",
    "    if num_imgs < num_lr:\n",
    "        psnr = PeakSignalNoiseRatio(reduction=None, dim=[1,2,3], data_range=255).to(device)\n",
    "        scores = psnr(lrs, hrs_res)\n",
    "        indices = torch.argsort(scores, descending=True)[:num_imgs]\n",
    "    else:\n",
    "        indices = torch.arange(num_lr, dtype=torch.uint8)\n",
    "\n",
    "    return hr.cpu(), lrs.cpu(), indices.cpu()\n",
    "\n",
    "def get_indice_from_pip(chip, tracker_path):\n",
    "    \"\"\"\n",
    "    Get the indice chosen from our pipeline.\n",
    "    \"\"\"\n",
    "    tracker = load_json(tracker_path)\n",
    "    return tracker[chip]\n",
    "\n",
    "# Hyperparameters of plotting\n",
    "font = 50\n",
    "num_col = 3\n",
    "\n",
    "def plot_one_row(hr, lrs, indices, chip, tracker_path):\n",
    "    \"\"\"\n",
    "    Plot the High resolution NAIP (left), the same High resolution after histigram matched with the low resolution \n",
    "    as reference (middle) and the low resolution (right) for each low-resolution\n",
    "    \"\"\"\n",
    "    m = len(indices)  \n",
    "    fig, axes = plt.subplots(m//num_col+1, num_col, figsize=(4.2 * (m + 1), 15*(m//num_col)))\n",
    "    \n",
    "    ind_pip = get_indice_from_pip(chip, tracker_path)\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis('off')\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    fig.suptitle(f'HR | HR (histogram matched) | LR\\nindice of pipeline : {ind_pip}', fontsize=2*font)\n",
    "\n",
    "    for i, ind in enumerate(indices):\n",
    "        lr_res = (F.interpolate(lrs[ind, :3, :, :].unsqueeze(0).float(), hr.shape[1:], mode='bicubic').squeeze(0))\n",
    "        hr_numpy = hr[:3].permute(1,2,0).numpy()\n",
    "        lr_numpy = lr_res.permute(1,2,0).int().numpy()\n",
    "        hr_matched = match_histograms(hr_numpy, lr_numpy, channel_axis=-1)\n",
    "        padding_tensor = np.ones((hr_numpy.shape[0], 2, 3))*255\n",
    "        to_plot = np.concatenate([\n",
    "            hr_numpy, \n",
    "            padding_tensor, \n",
    "            hr_matched,\n",
    "            padding_tensor,\n",
    "            lr_numpy\n",
    "            ], axis=1)\n",
    "        axes[i].imshow(np.clip(to_plot/255, a_min=0., a_max=1.))\n",
    "        axes[i].set_title(f'indice: {ind}', fontsize=font)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c73d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_picking(identifiers, rgb_dir, nir_dir, save_dir, tracker_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Function that wait for the user to input the right index for picking the low-resolution: \n",
    "        list_paths: List of paths, representing the pairs we want to manually pick the low-resolution\n",
    "        save_dir: Where to store the json output of the function that stores all the index picked by the user.\n",
    "        tracker_path: path of the pipeline tracker.json to get the pipeline index \n",
    "\n",
    "    Return:\n",
    "        json file consisting of a dictionnary: {\"unique_id/chip\": index_chosen_by_user} \n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for unique_id in identifiers:\n",
    "        hr, lrs = load_images(unique_id, rgb_dir, nir_dir)\n",
    "        hr, lrs, indices_cloud_removal = step_1_cloud_removal(hr, lrs, device)\n",
    "        plot_one_row(hr, lrs, indices_cloud_removal, unique_id, tracker_path)\n",
    "\n",
    "        plt.show()  \n",
    "        \n",
    "        best_indice = input('Enter the indice as in the plot (or type \"None\" if no acceptable image): ')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        if best_indice.strip().lower() == 'none':\n",
    "            best_indice_val = None\n",
    "        else:\n",
    "            try:\n",
    "                best_indice_val = int(best_indice)\n",
    "            except ValueError:\n",
    "                # If not an int and not \"None\", just keep the raw string or handle error as you prefer\n",
    "                best_indice_val = best_indice\n",
    "        \n",
    "        d[os.path.join(unique_id, chip)] = best_indice_val\n",
    "\n",
    "    with open(save_dir, 'w') as file:\n",
    "        json.dump(d, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac4b60",
   "metadata": {},
   "source": [
    "# Main Pipeline for manual picking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2560b",
   "metadata": {},
   "source": [
    "## Parameters\n",
    " * result_path: Path of the results \n",
    " * metric_name: metrics available [\"psnr_rgb\", \"psnr_all\", \"clip_rgb\", \"ssim_rgb\", \"ssim_all\", \"lpips_rgb\"], CLIP is Git-SCLIP\n",
    " * pipeline_directory: The path of the director where the pipeline images are stored\n",
    " * S2NAIP_directory:  The path of the director where the S2NAIP images are stored\n",
    " * nir_dir: directory of the nir band of NAIP images\n",
    " * json_save_path: path where to store the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddfd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"/workspace/framework_choose_lr/dev/compute_scores_test/results_scores/TEST_finetune_satlas_unet_hm_nir.json\" \n",
    "metric_name = \"psnr_rgb\" \n",
    "\n",
    "pipeline_directory = \"/workspace/pipeline-data/dataset_from_pipeline/ssim_0-7_hr-lr_0-3_hr-hrharm_NIR/test/\"\n",
    "pipeline_hr_directory = os.path.join(pipeline_directory, \"naip\") \n",
    "pipeline_lr_directory = os.path.join(pipeline_directory, \"sentinel2\") \n",
    "\n",
    "S2NAIP_directory = '/workspace/readonly-satlas/val_set'\n",
    "nir_dir = '/workspace/readonly-satlasNIR/nirres/val_set' \n",
    "\n",
    "tracker_path = os.path.join(pipeline_directory, \"tracker.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve scores :\n",
    "with open(results_path, 'r') as file:\n",
    "    score_results = json.load(file)\n",
    "\n",
    "# Extract psnr_rgb sorted : \n",
    "sorted_metric = extract_metrics(score_results, metric_name) \n",
    "\n",
    "# Retrieve the worst metric with respect to a threshold :\n",
    "metric_thresh = 17\n",
    "worst = get_worst_chips_by_threshold(sorted_metric, metric_thresh)\n",
    "# print(len(worst_17))  # 166\n",
    "\n",
    "identifiers = get_list_unique_id_chip(worst, os.path.join(S2NAIP_directory, 'naip'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6016bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We divise the work on sub-works\n",
    "slices = [\n",
    "    slice(0, 20),\n",
    "    slice(20, 40),\n",
    "    slice(40, 60),\n",
    "    slice(60, 80),\n",
    "    slice(80, 100),\n",
    "    slice(100, 120),\n",
    "    slice(120, 140),\n",
    "    slice(140, 160),\n",
    "    slice(160, 166) \n",
    "]\n",
    "i=1 # 0,1,2,3,4,5,6,7\n",
    "json_save_path = f'/workspace/framework_choose_lr/dev/Manual_test/manual_picking_json/manual_pick_{i*20}_{((i+1)*20)-1}.json'\n",
    "manual_picking(identifiers[slices[i]], S2NAIP_directory, nir_dir, json_save_path, tracker_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
